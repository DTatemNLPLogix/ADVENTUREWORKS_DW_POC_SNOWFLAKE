{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport os\nimport logging\nimport datetime\nfrom snowflake.snowpark.functions import when_matched, when_not_matched\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1a2be58e-e758-491b-be65-240bbd5459ee",
   "metadata": {
    "language": "python",
    "name": "Function_Declarations",
    "collapsed": false
   },
   "outputs": [],
   "source": "def fix_date_cols(df, tz = 'UTC'):\n    cols = df.select_dtypes(include=['datetime64[ns]']).columns\n    for col in cols:\n        df[col] = df[col].dt.tz_localize(tz)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10ca5b40-529a-4753-a2df-81daed9994c1",
   "metadata": {
    "language": "python",
    "name": "Logging",
    "collapsed": false
   },
   "outputs": [],
   "source": "logger = logging.getLogger('default')\n# if environment_level == 'NON-PROD':\n#     logger.setLevel(logging.DEBUG)    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c067b852-6be9-4ec0-b358-b857b0b9907a",
   "metadata": {
    "language": "python",
    "name": "Declarations",
    "collapsed": false
   },
   "outputs": [],
   "source": "stage_path = '@ingestion_staging'\ndatabase = 'ADVENTUREWORKS_DW'\nschema = 'BRONZE'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f457bc-21c4-49a4-9872-932de239b49a",
   "metadata": {
    "language": "python",
    "name": "Get_Staged_Files",
    "collapsed": false
   },
   "outputs": [],
   "source": "stage_table = session.sql(f\"\"\" LS {stage_path}\"\"\").collect()\nstage_table",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Get_Table_Names",
    "collapsed": false
   },
   "source": "stage_df =pd.DataFrame(stage_table)\ntables = []\nfor item in stage_df['name']:\n    table_name = item[17:]\n    table_name_parts = table_name.split('.')\n    tables.append((table_name_parts[1], table_name))\ntables\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e207120d-6d16-487c-b93d-18a8c459096b",
   "metadata": {
    "language": "python",
    "name": "Ingest_Data",
    "collapsed": false
   },
   "outputs": [],
   "source": "stage_path = '@ingestion_staging'\ntable_data={}\nfor item in tables:\n    table_data[item[0]] = session.read.options({\"FIELD_OPTIONALLY_ENCLOSED_BY\": '\"', \"PARSE_HEADER\": True, \"INFER_SCHEMA\" : True}).csv(stage_path + item[1])\n    # table_data[item[0]].show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b737f0ff-bd5b-4e1f-90fc-b28fa07a3f44",
   "metadata": {
    "language": "sql",
    "name": "Get_Tables",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT TABLE_SCHEMA, TABLE_NAME FROM ADVENTUREWORKS_DW.INFORMATION_SCHEMA.TABLES WHERE TABLES.TABLE_SCHEMA = 'BRONZE'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9ef4d30-4eb1-466c-b298-f3606daaa814",
   "metadata": {
    "language": "python",
    "name": "Check_For_Tables",
    "collapsed": false
   },
   "outputs": [],
   "source": "tables_exist_df = Get_Tables.to_pandas()\ntables_exist_df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e486dad-0ee6-4ee1-873f-41a8a696b0ab",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "print(table_data.keys())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6cfcde5-39ab-4e6a-9962-a87e23a8f74d",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "# if 'Territory' in tables_exist_df['TABLE_NAME'].values:\n#     print('true')\n# else:\n#     print('false')\n# print(table_data['Territory']['TerritoryID'])\n# target = session.table(database+ '.' + schema + '.' + '\"ProductCategory\"')\n# print(target['ProductCategoryID'])\nprint(table_data.keys())\n# print(table_data['Territory'].to_pandas().describe)\n# print(table_data['Territory']['\"TerritoryID\"'])\n# table_data['Product'].index\nprint(target.columns)\ntest_df = table_data['Product'].to_pandas().head()\ntype(test_df['SellStartDate'][0])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d678ff6e-86a3-43f6-add9-b5f236c2f35c",
   "metadata": {
    "language": "python",
    "name": "Create_Bronze_Tables_Or_Upsert",
    "collapsed": false
   },
   "outputs": [],
   "source": "for table in table_data.keys():\n    if table not in tables_exist_df['TABLE_NAME'].values:\n        write_df = table_data[table].to_pandas()\n        fix_date_cols(write_df)\n        obj = session.write_pandas( write_df,\n                                    table_name = table, \n                                    database = database,\n                                    schema = schema,\n                                    auto_create_table = True,\n                                    quote_identifiers = True,\n                                    use_logical_type = True)\n    else:\n        update_dict= {}\n        target = session.table(database+ '.' + schema + '.\"' + table+'\"')\n        cols = table_data[table].columns[1:]\n        # print(table_data[table].columns)\n        print(cols)\n        #print(target.columns)\n        # print(target.columns)\n        # print(target['\"'+ table + 'ID' + '\"'])\n        print(table_data[table]['\"'+ table + 'ID' + '\"'])\n\n        \n        for col in cols:\n            update_dict[col] = table_data[table][col]\n        print(update_dict)\n        target.merge(table_data[table], \n                     (target['\"'+ table + 'ID' + '\"'] == table_data[table]['\"'+ table + 'ID' + '\"']),\n              [when_matched().update(update_dict), #fixme build dict of all but id column matching to source\n              when_not_matched().insert({'\"'+ table + 'ID' + '\"':table_data[table]['\"'+ table + 'ID' + '\"']})\n              ])\n        #needs to have snowparkdf as source for merge, pandas as source for write, need way to ingest csv as snowpark tables",
   "execution_count": null
  }
 ]
}